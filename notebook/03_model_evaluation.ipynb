{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Model Evaluation\n",
    "\n",
    "This notebook provides comprehensive evaluation of trained medical image analysis models.\n",
    "\n",
    "## Contents\n",
    "1. Model Loading\n",
    "2. Performance Metrics\n",
    "3. Confusion Matrix Analysis\n",
    "4. ROC Curve Analysis\n",
    "5. Prediction Visualization\n",
    "6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Custom imports\n",
    "from data.data_loader import MedicalImageDataLoader\n",
    "from evaluation.metrics import ClassificationMetrics\n",
    "from utils.visualization import plot_confusion_matrix, visualize_predictions\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model_path = '../outputs/notebook_training/models/best_model.h5'\n",
    "\n",
    "if Path(model_path).exists():\n",
    "    model = keras.models.load_model(model_path)\n",
    "    print(f'Model loaded from {model_path}')\n",
    "    print(f'Model parameters: {model.count_params():,}')\n",
    "else:\n",
    "    print(f'Model not found at {model_path}')\n",
    "    print('Please run the training notebook first or update the path')\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data loader\n",
    "data_loader = MedicalImageDataLoader(\n",
    "    data_dir='../data/processed',\n",
    "    dataset_name='chest_xray',\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224),\n",
    "    num_classes=2,\n",
    "    augment=False  # No augmentation for evaluation\n",
    ")\n",
    "\n",
    "dataset_info = data_loader.get_dataset_info()\n",
    "class_names = dataset_info['class_names']\n",
    "print(f'Class names: {class_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Get predictions on test set\n",
    "    test_dataset = data_loader.create_tf_dataset('test')\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    \n",
    "    print('Generating predictions on test set...')\n",
    "    for batch_images, batch_labels in test_dataset:\n",
    "        predictions = model.predict(batch_images, verbose=0)\n",
    "        \n",
    "        y_true.extend(np.argmax(batch_labels, axis=1))\n",
    "        y_pred.extend(np.argmax(predictions, axis=1))\n",
    "        y_prob.extend(predictions)\n",
    "    \n",
    "    y_prob = np.array(y_prob)\n",
    "    \n",
    "    print(f'Evaluated {len(y_true)} samples')\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    metrics_calculator = ClassificationMetrics(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        y_prob=y_prob,\n",
    "        class_names=class_names\n",
    "    )\n",
    "    \n",
    "    all_metrics = metrics_calculator.calculate_all_metrics()\n",
    "    \n",
    "    # Display basic metrics\n",
    "    basic_metrics = all_metrics['basic_metrics']\n",
    "    print('\\nOverall Performance Metrics:')\n",
    "    print('='*40)\n",
    "    print(f'Accuracy: {basic_metrics["accuracy"]:.4f}')\n",
    "    print(f'Precision (Macro): {basic_metrics["precision_macro"]:.4f}')\n",
    "    print(f'Recall (Macro): {basic_metrics["recall_macro"]:.4f}')\n",
    "    print(f'F1-Score (Macro): {basic_metrics["f1_macro"]:.4f}')\n",
    "    \n",
    "    if all_metrics['auc_metrics']['auc'] is not None:\n",
    "        print(f'AUC: {all_metrics["auc_metrics"]["auc"]:.4f}')\n",
    "else:\n",
    "    print('Model not available for evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_true, y_pred, class_names, normalize=False, \n",
    "                         title='Confusion Matrix (Counts)')\n",
    "    \n",
    "    # Plot normalized confusion matrix\n",
    "    plot_confusion_matrix(y_true, y_pred, class_names, normalize=True, \n",
    "                         title='Confusion Matrix (Normalized)')\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print('\\nPer-Class Metrics:')\n",
    "    print('='*40)\n",
    "    per_class_metrics = all_metrics['per_class_metrics']\n",
    "    for class_name, metrics in per_class_metrics.items():\n",
    "        print(f'{class_name}:')\n",
    "        print(f'  Precision: {metrics["precision"]:.4f}')\n",
    "        print(f'  Recall: {metrics["recall"]:.4f}')\n",
    "        print(f'  F1-Score: {metrics["f1_score"]:.4f}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and y_prob is not None:\n",
    "    from evaluation.metrics import MedicalImageMetrics\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    MedicalImageMetrics.plot_roc_curve(\n",
    "        np.array(y_true), y_prob, class_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Visualize correct predictions\n",
    "    print('Correct Predictions:')\n",
    "    visualize_predictions(model, data_loader, split='test', num_samples=8)\n",
    "    \n",
    "    # Find and visualize incorrect predictions\n",
    "    incorrect_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) \n",
    "                        if true != pred]\n",
    "    \n",
    "    if incorrect_indices:\n",
    "        print(f'\\nFound {len(incorrect_indices)} incorrect predictions')\n",
    "        \n",
    "        # Get some incorrect samples for visualization\n",
    "        sample_incorrect = incorrect_indices[:8]\n",
    "        \n",
    "        # This would require modifying the visualization function\n",
    "        # to accept specific indices - simplified here\n",
    "        print('Sample incorrect predictions would be shown here')\n",
    "    else:\n",
    "        print('No incorrect predictions found in sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Analyze prediction confidence\n",
    "    max_probs = np.max(y_prob, axis=1)\n",
    "    correct_predictions = (np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    # Confidence distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Overall confidence distribution\n",
    "    axes[0].hist(max_probs, bins=30, alpha=0.7, color='blue')\n",
    "    axes[0].set_title('Prediction Confidence Distribution')\n",
    "    axes[0].set_xlabel('Max Probability')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].axvline(np.mean(max_probs), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(max_probs):.3f}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Confidence by correctness\n",
    "    correct_conf = max_probs[correct_predictions]\n",
    "    incorrect_conf = max_probs[~correct_predictions]\n",
    "    \n",
    "    axes[1].hist(correct_conf, bins=20, alpha=0.7, label='Correct', color='green')\n",
    "    axes[1].hist(incorrect_conf, bins=20, alpha=0.7, label='Incorrect', color='red')\n",
    "    axes[1].set_title('Confidence by Prediction Correctness')\n",
    "    axes[1].set_xlabel('Max Probability')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print('Confidence Analysis:')\n",
    "    print('='*30)\n",
    "    print(f'Mean confidence (correct): {np.mean(correct_conf):.4f}')\n",
    "    print(f'Mean confidence (incorrect): {np.mean(incorrect_conf):.4f}')\n",
    "    print(f'Overall accuracy: {np.mean(correct_predictions):.4f}')\n",
    "    \n",
    "    # Low confidence predictions\n",
    "    low_conf_threshold = 0.7\n",
    "    low_conf_indices = np.where(max_probs < low_conf_threshold)[0]\n",
    "    print(f'\\nLow confidence predictions (< {low_conf_threshold}): {len(low_conf_indices)}')\n",
    "    \n",
    "    if len(low_conf_indices) > 0:\n",
    "        low_conf_accuracy = np.mean(correct_predictions[low_conf_indices])\n",
    "        print(f'Accuracy on low confidence predictions: {low_conf_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This evaluation notebook provided:\n",
    "- Comprehensive performance metrics\n",
    "- Confusion matrix analysis\n",
    "- ROC curve visualization\n",
    "- Prediction confidence analysis\n",
    "- Error pattern identification\n",
    "\n",
    "Use these insights to:\n",
    "- Understand model strengths and weaknesses\n",
    "- Identify areas for improvement\n",
    "- Make informed decisions about model deployment\n",
    "- Guide future data collection and model refinement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}